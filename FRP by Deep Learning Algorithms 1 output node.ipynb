{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create Variables per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_to_np (df):\n",
    "    # Team Strength\n",
    "    # Through SPI \n",
    "    \n",
    "    # Cumulative values\n",
    "    away_items = ['AC', 'AF', 'AR', 'AS', 'AST', 'AY','FTAG']\n",
    "    home_items = ['HC', 'HF', 'HR', 'HS', 'HST', 'HY','FTHG']\n",
    "    for item in home_items:\n",
    "        df[(\"C_\"+str(item))] = (df.groupby('HomeTeam')[item].transform(pd.Series.cumsum))-df[item]\n",
    "    for item in away_items:\n",
    "        df[(\"C_\"+str(item))] = (df.groupby('AwayTeam')[item].transform(pd.Series.cumsum))-df[item]\n",
    "    \n",
    "    # Map result to points\n",
    "    df['FTHP'] = df['FTR'].map({'H': 3, 'D': 1, \"A\": 0})\n",
    "    df['FTAP'] = df['FTR'].map({'H': 0, 'D': 1, \"A\": 3})\n",
    "    \n",
    "    # Cumulative points\n",
    "    df[(\"C_FTHP\")] = (df.groupby('HomeTeam')['FTHP'].transform(pd.Series.cumsum))-df['FTHP']\n",
    "    df[(\"C_FTAP\")] = (df.groupby('AwayTeam')['FTAP'].transform(pd.Series.cumsum))-df['FTAP'] \n",
    "    \n",
    "    # Form\n",
    "    # Form Home Team for Home matches\n",
    "    df['F_HT'] = df.groupby('HomeTeam', as_index=False)['FTHP'].rolling(5).sum().reset_index(0, drop=True)-df['FTHP']\n",
    "    # Form Away Team for Away matches\n",
    "    df['F_AT'] = df.groupby('AwayTeam', as_index=False)['FTAP'].rolling(5).sum().reset_index(0, drop=True)-df['FTAP']\n",
    "\n",
    "    # Difference between matches\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\n",
    "    df['D_HM'] = df.groupby(['HomeTeam'])['Date'].diff().fillna(0).dt.days#.astype(int)\n",
    "    df['D_AM'] = df.groupby('AwayTeam')['Date'].diff().fillna(0).dt.days#.astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leagues = [\" Premier League\", \" Championship\", \" League 1\", \" League 2\"]\n",
    "seasons = [\"Season \"+str(i)+\" \"+str(i+1) for i in range(2006,2018,1)]\n",
    "all_files = []\n",
    "\n",
    "for season in seasons:\n",
    "    for league in leagues:\n",
    "        all_files.append(\"files/\"+str(season)+str(league)+\".csv\")\n",
    "#print(len(all_files))\n",
    "#print(all_files[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Season & Div column to all files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with Season\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['Season']=str(file)\n",
    "    df.to_csv(file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with Division\n",
    "for file in all_files:\n",
    "    for league in leagues:\n",
    "        if league in file:\n",
    "            df = pd.read_csv(file)\n",
    "            df['Div']=league\n",
    "            df.to_csv(file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineer, concat & Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for filename in all_files[1:25]:\n",
    "    df = season_to_np(pd.read_csv(filename, index_col=0))#header=0\n",
    "    li.append(df)\n",
    "    # till 25 works well then mismatch\n",
    "    \n",
    "for filename in all_files[25:40]:\n",
    "    df = season_to_np(pd.read_csv(filename, index_col=0, skipfooter=2, error_bad_lines=False, engine='python'))#header=0\n",
    "    li.append(df) \n",
    "    #Mismatched solved by skipfooter and error_bad_lines\n",
    "    \n",
    "for filename in all_files[40:]:\n",
    "    df = season_to_np(pd.read_csv(filename, index_col=0))#header=0\n",
    "    li.append(df)\n",
    "    #works again from 40:\n",
    "\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True, sort=True)\n",
    "df.to_csv(\"all_files.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add column with team strength / SPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all_files.csv\")\n",
    "print(data.shape)\n",
    "spi = pd.read_csv(\"spi_england1\", index_col='name', usecols=['name', 'spi'])\n",
    "data = data.join(spi, on = 'HomeTeam', rsuffix='_H')\n",
    "data = data.join(spi, on = 'AwayTeam', rsuffix='_A')\n",
    "\n",
    "data.to_csv(\"all_files_spi.csv\", index=False)\n",
    "print(data.shape)\n",
    "#data[(\"Form_FTHP\")] = data.groupby('HomeTeam')['FTHP'].rolling(7).sum()\n",
    "#data[('Form_FTHP')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorize & select required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24029, 104)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_files_spi.csv\")\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Distribution\")\n",
    "#print()\n",
    "\n",
    "#for league in leagues:\n",
    "#    subl = df['Div'] == league\n",
    "#    dfl = df[subl]\n",
    "#    print(league)\n",
    "#    print(dfl[\"FTR\"].value_counts())\n",
    "#    print ()\n",
    "    #print(df[df['Season']==league]\n",
    "    #print(df[filt][\"FTR\"].value_counts())\n",
    "    #print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = pd.factorize(df['FTR'])[0]\n",
    "#df['HomeTeam'] = pd.factorize(df['HomeTeam'])[0]\n",
    "#df['AwayTeam'] = pd.factorize(df['AwayTeam'])[0]\n",
    "df['Referee'] = pd.factorize(df['Referee'])[0]\n",
    "df['Div'] = pd.factorize(df['Div'])[0]\n",
    "# 0 = premier league\n",
    "df['Season'] = pd.factorize(df['Season'])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Season', 'Div', 'Referee', 'B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'WHH', 'WHD', 'WHA', 'VCH', 'VCD', 'VCA', 'C_AC', 'C_AF', 'C_AR', 'C_AS', 'C_AST', 'C_AY', 'C_FTAG', 'spi_A', 'C_FTAP', 'F_AT', 'C_HC', 'C_HF', 'C_HR', 'C_HS', 'C_HST', 'C_HY', 'C_FTHG', 'spi', 'C_FTHP', 'F_HT', 'label']\n",
      "<bound method Index.unique of Index(['Season', 'Div', 'Referee', 'B365H', 'B365D', 'B365A', 'BWH', 'BWD',\n",
      "       'BWA', 'IWH', 'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'WHH', 'WHD', 'WHA',\n",
      "       'VCH', 'VCD', 'VCA', 'C_AC', 'C_AF', 'C_AR', 'C_AS', 'C_AST', 'C_AY',\n",
      "       'C_FTAG', 'spi_A', 'C_FTAP', 'F_AT', 'C_HC', 'C_HF', 'C_HR', 'C_HS',\n",
      "       'C_HST', 'C_HY', 'C_FTHG', 'spi', 'C_FTHP', 'F_HT', 'label'],\n",
      "      dtype='object')>\n"
     ]
    }
   ],
   "source": [
    "req_columns =[ #team indep 19 features \n",
    "                'Season', 'Div',#'HomeTeam', 'AwayTeam',\n",
    "    'Referee',\n",
    "              \n",
    "              #Bookmakers x 6 = 18 features\n",
    "              'B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', \n",
    "              'LBH', 'LBD', 'LBA', 'WHH', 'WHD', 'WHA',  \n",
    "              'VCH', 'VCD', 'VCA', \n",
    "              \n",
    "\n",
    "              \n",
    "              # Team dependent feat\n",
    "              # Away Team\n",
    "              'C_AC', 'C_AF', 'C_AR', 'C_AS', 'C_AST', 'C_AY', 'C_FTAG', \"spi_A\",'C_FTAP','F_AT',\n",
    "              \n",
    "              # Home Team\n",
    "              'C_HC', 'C_HF', 'C_HR', 'C_HS', 'C_HST', 'C_HY', 'C_FTHG', \"spi\",'C_FTHP', 'F_HT',\n",
    "              \n",
    "              # Current match --> 'FTHP', 'FTAP',\n",
    "              \n",
    "            \n",
    "            # \"spi_A\",\"spi\",\n",
    "               'label']\n",
    "     \n",
    "\n",
    "df = df[req_columns]\n",
    "print(req_columns)\n",
    "print(df.columns.unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24029, 42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dd to Np & Fill NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True) # data = np.array(df)\n",
    "data = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24029, 42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = data[:,:-1] / (data[:,:-1].max(axis=0))\n",
    "#data = np.nan_to_num(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24029, 42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, shuffle=True) \n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train, y_train, test_size=0.25, random_state=42,shuffle=True)\n",
    "\n",
    "# In this way, train, val, test set will be 60%, 20%, 20% of the dataset respectively.\n",
    "\n",
    "x_len = X_train.shape[1]\n",
    "\n",
    "tot = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/stefansamba/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# MLP Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "\n",
    "# Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(75, activation = 'relu', input_dim = x_len))\n",
    "\n",
    "\n",
    "# Adding the second hidden layer\n",
    "#model.add(Dense(units = 10, activation = 'relu'))\n",
    "#model.add(Dense(units = 10, activation = 'relu'))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "#model.add(Dense(units = 3))\n",
    "model.add(Dense(units = 1, activation='relu'))\n",
    "#model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)  \n",
    "\n",
    "model.compile(optimizer = sgd, loss = 'mean_squared_error', metrics=['accuracy'])\n",
    "# layers (6 tot 15 trial )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/stefansamba/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 14417 samples, validate on 4806 samples\n",
      "Epoch 1/50\n",
      "14417/14417 [==============================] - 1s 45us/step - loss: 1.9193 - acc: 0.4360 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 2/50\n",
      "14417/14417 [==============================] - 1s 45us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 3/50\n",
      "14417/14417 [==============================] - 1s 38us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 4/50\n",
      "14417/14417 [==============================] - 1s 38us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 5/50\n",
      "14417/14417 [==============================] - 1s 40us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 6/50\n",
      "14417/14417 [==============================] - 1s 40us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 7/50\n",
      "14417/14417 [==============================] - 1s 58us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 8/50\n",
      "14417/14417 [==============================] - 2s 107us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 9/50\n",
      "14417/14417 [==============================] - 1s 76us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 10/50\n",
      "14417/14417 [==============================] - 1s 51us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 11/50\n",
      "14417/14417 [==============================] - 1s 50us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 12/50\n",
      "14417/14417 [==============================] - 1s 57us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 13/50\n",
      "14417/14417 [==============================] - 1s 43us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 14/50\n",
      "14417/14417 [==============================] - 0s 34us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 15/50\n",
      "14417/14417 [==============================] - 1s 47us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 16/50\n",
      "14417/14417 [==============================] - 1s 35us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 17/50\n",
      "14417/14417 [==============================] - 1s 42us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 18/50\n",
      "14417/14417 [==============================] - 1s 41us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 19/50\n",
      "14417/14417 [==============================] - 1s 44us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 20/50\n",
      "14417/14417 [==============================] - 0s 30us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 21/50\n",
      "14417/14417 [==============================] - 1s 39us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 22/50\n",
      "14417/14417 [==============================] - 0s 33us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 23/50\n",
      "14417/14417 [==============================] - 1s 43us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 24/50\n",
      "14417/14417 [==============================] - 0s 33us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 25/50\n",
      "14417/14417 [==============================] - 0s 31us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 26/50\n",
      "14417/14417 [==============================] - 0s 30us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 27/50\n",
      "14417/14417 [==============================] - 0s 31us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 28/50\n",
      "14417/14417 [==============================] - 0s 31us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 29/50\n",
      "14417/14417 [==============================] - 1s 45us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 30/50\n",
      "14417/14417 [==============================] - 0s 32us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 31/50\n",
      "14417/14417 [==============================] - 0s 33us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 32/50\n",
      "14417/14417 [==============================] - 0s 30us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 33/50\n",
      "14417/14417 [==============================] - 0s 33us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 34/50\n",
      "14417/14417 [==============================] - 1s 41us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 35/50\n",
      "14417/14417 [==============================] - 0s 30us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 36/50\n",
      "14417/14417 [==============================] - 1s 35us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 37/50\n",
      "14417/14417 [==============================] - 1s 36us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 38/50\n",
      "14417/14417 [==============================] - 0s 30us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 39/50\n",
      "14417/14417 [==============================] - 0s 31us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 40/50\n",
      "14417/14417 [==============================] - 0s 31us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 41/50\n",
      "14417/14417 [==============================] - 1s 38us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 42/50\n",
      "14417/14417 [==============================] - 1s 51us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 43/50\n",
      "14417/14417 [==============================] - 2s 107us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 44/50\n",
      "14417/14417 [==============================] - 1s 74us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 45/50\n",
      "14417/14417 [==============================] - 2s 135us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 46/50\n",
      "14417/14417 [==============================] - 1s 80us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 47/50\n",
      "14417/14417 [==============================] - 1s 62us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 48/50\n",
      "14417/14417 [==============================] - 0s 32us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 49/50\n",
      "14417/14417 [==============================] - 1s 52us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n",
      "Epoch 50/50\n",
      "14417/14417 [==============================] - 1s 47us/step - loss: 1.3604 - acc: 0.4366 - val_loss: 1.3689 - val_acc: 0.4395\n"
     ]
    }
   ],
   "source": [
    "fitted = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_mlp1.h5')  # creates a HDF5 file 'my_model.h5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "loss = scores[0]\n",
    "accuracy = scores[1]\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Loss = \\t\\t{}\".format(loss))\n",
    "print(\"Accuracy = \\t{}\".format(accuracy))\n",
    "print(\"Random \\t= \\t0.33\")\n",
    "print(\"On Distr = \\t{}\".format(df.label.value_counts()[0]/sum(df.label.value_counts())*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fitted \n",
    "   \n",
    "print(history.history.keys())  \n",
    "   \n",
    "plt.figure(1)  \n",
    "   \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'],color ='black',linestyle='dashed')  \n",
    "plt.plot(history.history['val_acc'],color ='grey',linestyle='solid')  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')  \n",
    "#plt.savefig('model-6-1.png')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # summarize history for loss  \n",
    "\n",
    "plt.subplot(212)  \n",
    "\n",
    "plt.plot(history.history['loss'],color ='black',linestyle='dashed')  \n",
    "plt.plot(history.history['val_loss'],color ='grey',linestyle='solid')  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')  \n",
    "#plt.show()  \n",
    "#plt.savefig('model-6-2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
